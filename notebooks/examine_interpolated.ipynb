{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Examining Physics Using the Interpolation Method\n",
    "\n",
    "This notebook demonstrates the final phase of Spyral, solving. To run this notebook, the previous three phases (point cloud, cluster, estimate) of Spyral *must* have been run on the data. Now that we have generated our clusters and estimated the physical observables of interest we are ready to initiate the solving phase of the analysis, where we attempt to extract the exact physics observables by fitting solutions of the equations of motion to the data. There are several approaches to solving in this application, and this notebook looks at what is referred to as the interpolation method. The interpolation method works by pre-generating a bunch of solutions to the ODE's and then interpolation on these solutions to try and fit the data. It has the advantage of being very fast; the ODE's only ever need to be solved once, and then all the remaining calculation is just simple (bi)linear interpolation.\n",
    "\n",
    "First let's take care of all of our imports.\n",
    "\n",
    "Note: this notebook assumes you've already done the work of generating the interpolation scheme. This is done by simply running phase 4 in the main application with an interpolation scheme defined. This can take some time (20-30 min depending on the coarse-ness of the grid)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spyral.core.cluster import Cluster\n",
    "from spyral.interpolate.track_interpolator import create_interpolator\n",
    "from spyral.solvers.solver_interp import fit_model_interp, Guess, interpolate_trajectory\n",
    "from spyral.core.run_stacks import form_run_string\n",
    "from spyral import SolverParameters, DetectorParameters, InterpSolverPhase\n",
    "\n",
    "from spyral_utils.nuclear import NuclearDataMap\n",
    "from spyral_utils.nuclear.particle_id import deserialize_particle_id\n",
    "\n",
    "import polars as pl\n",
    "import numpy as np\n",
    "import h5py as h5\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now with all of our code imported we will setup the configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set some parameters\n",
    "workspace_path = Path(\"/path/to/your/workspace/\")\n",
    "\n",
    "solver_params = SolverParameters(\n",
    "    gas_data_path=Path(\"/path/to/some/gas/data.json\"),\n",
    "    particle_id_filename=Path(\"/path/to/some/particle/id.json\"),\n",
    "    ic_min_val=900.0,\n",
    "    ic_max_val=1350.0,\n",
    "    n_time_steps=10000,\n",
    "    interp_ke_min=0.05,\n",
    "    interp_ke_max=70.0,\n",
    "    interp_ke_bins=400,\n",
    "    interp_polar_min=2.0,\n",
    "    interp_polar_max=88.0,\n",
    "    interp_polar_bins=170,\n",
    ")\n",
    "\n",
    "det_params = DetectorParameters(\n",
    "    magnetic_field=2.85,\n",
    "    electric_field=45000.0,\n",
    "    detector_length=1000.0,\n",
    "    beam_region_radius=25.0,\n",
    "    micromegas_time_bucket=10.0,\n",
    "    window_time_bucket=560.0,\n",
    "    get_frequency=6.25,\n",
    "    garfield_file_path=Path(\"/path/to/some/garfield.txt\"),\n",
    "    do_garfield_correction=False,\n",
    ")\n",
    "\n",
    "cluster_path = workspace_path / \"Cluster\" # this may change if you add custom phases\n",
    "estimate_path = workspace_path / \"Estimation\" # this may change if you add custom phases"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we need to load our interpolation mesh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nuc_map = NuclearDataMap()\n",
    "pid = deserialize_particle_id(solver_params.particle_id_filename, nuc_map)\n",
    "if pid is None:\n",
    "    raise Exception(\"Particle ID error!\")\n",
    "solver = InterpSolverPhase(solver_params, det_params)\n",
    "success = solver.create_assets(workspace_path)\n",
    "if not success:\n",
    "    raise Exception(\"Could not setup interpolation mesh!\")\n",
    "tracks = create_interpolator(solver.track_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we'll pick a data file (the min run number defined in the config by default) and load up the associated physics estimates. By default we select a random row in the estimates (a random cluster/trajectory) but you can always set the row to a hardcoded number to do some testing of parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_number = 16\n",
    "cluster_file_path = cluster_path / f\"{form_run_string(run_number)}.h5\"\n",
    "cluster_file = h5.File(cluster_file_path, \"r\")\n",
    "estimate_file_path = estimate_path / f\"{form_run_string(run_number)}.parquet\"\n",
    "estimate_df = pl.scan_parquet(estimate_file_path)\n",
    "estimate_gated = estimate_df.filter(pl.struct(['dEdx', 'brho']).map_batches(pid.cut.is_cols_inside)).collect().to_dict()\n",
    "cluster_group = cluster_file['cluster']\n",
    "nrows = len(estimate_gated['event'])\n",
    "row = np.random.randint(0, nrows)\n",
    "row = 6990\n",
    "print(f'row: {row}')\n",
    "event = estimate_gated['event'][row]\n",
    "cluster_index = estimate_gated['cluster_index'][row]\n",
    "print(f'event: {event}')\n",
    "print(f'cluster index: {cluster_index}')\n",
    "event_group = cluster_group[f'event_{event}']\n",
    "local_cluster = event_group[f'cluster_{cluster_index}']\n",
    "print(f'Direction: {estimate_gated[\"direction\"][row]}')\n",
    "cluster = Cluster(event, local_cluster.attrs['label'], local_cluster['cloud'][:].copy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With our cluster and estimated observables loaded, we are ready to fit to the data. We setup our Guess object from our estimates and then pass that along to the fit_model function. Sometimes this will return None when a given trajectory has estimates that are outside the interpolation table (these typically correspond to bad events). If this happens a error will occur. Simply re-run the notebook until the a good event is randomly selected. Note that the first time you run this block it might take a couple of seconds. This is because the interpolation method uses a just-in-time compiler (jit) to speed up the calculations. The first time you call the code, the code gets compiled (resulting in a slowdown). But everytime the code is called after that, the compiled program is used, resulting in enormus performance gains."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "guess = Guess(estimate_gated['polar'][row], estimate_gated['azimuthal'][row], estimate_gated['brho'][row], estimate_gated['vertex_x'][row], estimate_gated['vertex_y'][row], estimate_gated['vertex_z'][row])\n",
    "print(guess)\n",
    "result = fit_model_interp(cluster, guess, pid.nucleus, tracks, det_params)\n",
    "if result is None:\n",
    "    print('Guess outside of interpolation range!')\n",
    "best_fit_trajectory = interpolate_trajectory(result, tracks, pid.nucleus)\n",
    "cluster.data[:, :3] *= 0.001"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If a good event was chosen, you should see above a print out of the fit results. Key values are the chi-square (which should be small) and the variable values, which are the fitted observables. Also important are the correlations, which tell you if any of the parameters are co-dependent. If two parameters have a correlation of 1.0 they are basically degenerate to the fitter, which is very bad.\n",
    "\n",
    "We can also plot the results of the fit against the data to vizualize the performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = make_subplots(2, 2, subplot_titles=[\"XY Projection\", \"XZ Projection\", \"YZ Projection\"], specs=[[{\"rowspan\": 2}, {}],[None, {}]])\n",
    "fig.add_trace(\n",
    "    go.Scatter(\n",
    "        x=cluster.data[:, 0],\n",
    "        y=cluster.data[:, 1],\n",
    "        mode=\"markers\", \n",
    "        marker={\n",
    "            \"size\": 3,\n",
    "            \"color\": \"blue\"\n",
    "        },\n",
    "        name=\"Data\"\n",
    "    ),\n",
    "    row=1,\n",
    "    col=1\n",
    ")\n",
    "fig.add_trace(\n",
    "    go.Scatter(\n",
    "        x=best_fit_trajectory[:, 0],\n",
    "        y=best_fit_trajectory[:, 1],\n",
    "        mode=\"markers\",\n",
    "        marker={\n",
    "            \"size\": 3,\n",
    "            \"color\": \"red\"\n",
    "        },\n",
    "        name=\"Fit\"\n",
    "    ),\n",
    "    row=1,\n",
    "    col=1\n",
    ")\n",
    "fig.add_trace(\n",
    "    go.Scatter(\n",
    "        x=[result[\"vertex_x\"]],\n",
    "        y=[result[\"vertex_y\"]],\n",
    "        mode=\"markers\",\n",
    "        marker={\n",
    "            \"color\": \"green\",\n",
    "            \"size\": 4\n",
    "        },\n",
    "        name=\"Fit Vertex\"\n",
    "    ),\n",
    "    row=1,\n",
    "    col=1\n",
    ")\n",
    "\n",
    "fig.add_trace(\n",
    "    go.Scatter(\n",
    "        x=cluster.data[:, 2],\n",
    "        y=cluster.data[:, 0],\n",
    "        mode=\"markers\",\n",
    "        marker={\n",
    "            \"size\": 3,\n",
    "            \"color\": \"blue\"\n",
    "        },\n",
    "        name=\"Data\",\n",
    "        showlegend=False\n",
    "    ),\n",
    "    row=1,\n",
    "    col=2\n",
    ")\n",
    "fig.add_trace(\n",
    "    go.Scatter(\n",
    "        x=best_fit_trajectory[:, 2],\n",
    "        y=best_fit_trajectory[:, 0],\n",
    "        mode=\"markers\",\n",
    "        marker={\n",
    "            \"size\": 3,\n",
    "            \"color\": \"red\"\n",
    "        },\n",
    "        name=\"Fit\",\n",
    "        showlegend=False\n",
    "    ),\n",
    "    row=1,\n",
    "    col=2\n",
    ")\n",
    "fig.add_trace(\n",
    "    go.Scatter(\n",
    "        x=[result[\"vertex_z\"]],\n",
    "        y=[result[\"vertex_x\"]],\n",
    "        mode=\"markers\",\n",
    "        marker={\n",
    "            \"color\": \"green\",\n",
    "            \"size\": 4\n",
    "        },\n",
    "        name=\"Fit Vertex\",\n",
    "        showlegend=False,\n",
    "    ),\n",
    "    row=1,\n",
    "    col=2\n",
    ")\n",
    "\n",
    "fig.add_trace(\n",
    "    go.Scatter(\n",
    "        x=cluster.data[:, 2],\n",
    "        y=cluster.data[:, 1],\n",
    "        mode=\"markers\",\n",
    "        marker={\n",
    "            \"size\": 3,\n",
    "            \"color\": \"blue\"\n",
    "        },\n",
    "        name=\"Data\",\n",
    "        showlegend=False,\n",
    "    ),\n",
    "    row=2,\n",
    "    col=2\n",
    ")\n",
    "fig.add_trace(\n",
    "    go.Scatter(\n",
    "        x=best_fit_trajectory[:, 2],\n",
    "        y=best_fit_trajectory[:, 1],\n",
    "        mode=\"markers\",\n",
    "        marker={\n",
    "            \"size\": 3,\n",
    "            \"color\": \"red\"\n",
    "        },\n",
    "        name=\"Fit\",\n",
    "        showlegend=False,\n",
    "    ),\n",
    "    row=2,\n",
    "    col=2\n",
    ")\n",
    "fig.add_trace(\n",
    "    go.Scatter(\n",
    "        x=[result[\"vertex_z\"]],\n",
    "        y=[result[\"vertex_y\"]],\n",
    "        mode=\"markers\",\n",
    "        marker={\n",
    "            \"color\": \"green\",\n",
    "            \"size\": 4\n",
    "        },\n",
    "        name=\"Fit Vertex\",\n",
    "        showlegend=False\n",
    "    ),\n",
    "    row=2,\n",
    "    col=2\n",
    ")\n",
    "\n",
    "fig[\"layout\"][\"xaxis1\"][\"title\"] = \"X (m)\"\n",
    "fig[\"layout\"][\"xaxis1\"][\"range\"] = [-0.3, 0.3]\n",
    "fig[\"layout\"][\"yaxis1\"][\"title\"] = \"Y (m)\"\n",
    "fig[\"layout\"][\"yaxis1\"][\"range\"] = [-0.3, 0.3]\n",
    "\n",
    "fig[\"layout\"][\"xaxis2\"][\"title\"] = \"Z (m)\"\n",
    "fig[\"layout\"][\"xaxis2\"][\"range\"] = [0.0, 1.0]\n",
    "fig[\"layout\"][\"yaxis2\"][\"title\"] = \"X (m)\"\n",
    "fig[\"layout\"][\"yaxis2\"][\"range\"] = [-0.3, 0.3]\n",
    "\n",
    "fig[\"layout\"][\"xaxis3\"][\"title\"] = \"Z (m)\"\n",
    "fig[\"layout\"][\"xaxis3\"][\"range\"] = [0.0, 1.0]\n",
    "fig[\"layout\"][\"yaxis3\"][\"title\"] = \"Y (m)\"\n",
    "fig[\"layout\"][\"yaxis3\"][\"range\"] = [-0.3, 0.3]\n",
    "\n",
    "fig.update_layout(\n",
    "    width=1450,\n",
    "    height=725\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hopefully you see a nice fit to the data! If the fit looks bad, there are several things to check. First is the particle ID gate; if the wrong particle group is selected, the fit will fail spectacularly. Another is the coarse-ness of the interpolation scheme. If there are too few bins in the polar angle or the particle kinetic energy, the interpolation may not generate good values. Finally, it is also good to make sure that the target gas is correctly defined with the right pressure and chemistry."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
